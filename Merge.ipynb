{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0bb03fd57f0abe20aaa44405faada2d45c76eeb8f5b4a3cb120ceb3cb1a26d008",
   "display_name": "Python 3.9.2 64-bit ('bachelor_thesis': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import followthemoney as ftm\n",
    "import followthemoney_enrich as ftm_enrich\n",
    "import followthemoney.model as model\n",
    "from followthemoney.dedupe import Match, Linker\n",
    "import json\n",
    "import pandas as pd\n",
    "import gdown"
   ]
  },
  {
   "source": [
    "# Overview\n",
    "\n",
    "After having assigned an identifier to an entity via the PoC, we want to integrate two different collections in order to form one big interlinked graph. This corresponds to step 2 in the architectural overview.\n",
    "\n",
    "!<img src=\"./img/architecture.JPG\" width=\"400\" />\n",
    "\n",
    "For this evaluation, this is done with the [everypolitician](http://everypolitician.org) dataset, which has been mapped to the FtM ontology by an existing [scraper](https://github.com/pudo/opensanctions/blob/main/opensanctions/crawlers/everypolitician.py).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Get Data\n",
    "In a normal setting, one would load collection by using the CLI.\n",
    "\n",
    "```\n",
    "alephclient --host https://aleph.occrp.org --api-key <api-key> stream-entities -f <collection-id> -o <outfile>\n",
    "```\n",
    "\n",
    "For reproducibility, this has been done in advance.\n",
    " \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/output/\"\n",
    "ep_path = path + \"everypolitician.json\"\n",
    "ma_path = path + \"meineabgeordneten_wikidata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1YNQKfm6qLKb5M6cfNrk9m8UYwi4iOxdF\n",
      "To: /home/peter/dev/evaluation_bachelor_thesis/data/output/everypolitician.json\n",
      "127MB [00:09, 13.1MB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./data/output/everypolitician.json'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "gdown.download(\"https://drive.google.com/u/0/uc?id=1YNQKfm6qLKb5M6cfNrk9m8UYwi4iOxdF\", ep_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ftm_json(path, filter = \"Person\"):\n",
    "    entity_dict = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            entity = model.get_proxy(json.loads(line))\n",
    "            wd = entity.first(\"wikidataId\", True)\n",
    "            if entity.schema.name == filter:\n",
    "                if wd:\n",
    "                    entity_dict[wd] = entity\n",
    "    return entity_dict\n",
    "path = \"./data/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mein_abg = read_ftm_json(ma_path)\n",
    "every_polit = read_ftm_json(ep_path)"
   ]
  },
  {
   "source": [
    "# Matching\n",
    "We will check for equal Wikidata IDs and create a Match object. A match objects holds two entity IDs and a decision about the sameness. This match object could be uploaded to Aleph via the API. However, we will use it to perfom it instantly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enricher = ftm_enrich.enricher.Enricher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, polit in every_polit.items():\n",
    "    abg = mein_abg.get(idx)\n",
    "    if abg:\n",
    "        #print(polit.to_dict())\n",
    "        match = enricher.make_match(abg, polit)\n",
    "        match = Match(model, {})\n",
    "        match.entity=  abg\n",
    "        match.canonical =  polit\n",
    "        match.decision = match.SAME\n",
    "        matches.append(match)\n"
   ]
  },
  {
   "source": [
    "In total, there are 77 matching person entities with respect to the wikidata ID."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "source": [
    "# Merging\n",
    "The merging logic actually exists in the ftm [repository](https://github.com/alephdata/followthemoney/blob/6cb55e319f69443dff17bf1ee5dd1a37a31b5c4a/followthemoney/cli/dedupe.py) and works the following:\n",
    "\n",
    "1. Create a linker object, which takes match objects and checks if there is a sameness decision.\n",
    "2. If so, add the pair to a hashmap (Python dict) in the linker object.\n",
    "3. Iterate through both collection of to-be-merged entities and pass each entity to the linker object (which knows the links). If the entity's ID is stored in the hashmap, adopt the entity ID. If not, keep the ID. This also applies for \"edges\", such as memberships.\n",
    "4. Write to file. \n",
    "5. As we have duplicates, we aggregate, which merges items with the same ID. Merging just unions both, properties and their values. Therefore, same properties are merged, and different ones are just added to the multi-valued list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Example on how it works"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'a': '891bd4dbcf5506d489f8d6e757ace9411eccee55',\n",
       " 'b': 'a21072d75aebf5f72865a70ca9e10beffb9ddb27',\n",
       " 'merged': '891bd4dbcf5506d489f8d6e757ace9411eccee55',\n",
       " 'result': {'name': ['Prof. Dr. hans kelsen', 'hans kelsen'],\n",
       "  'title': ['Dr', 'Prof.'],\n",
       "  'birthDate': ['1908', '1908-07-06']}}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "linker_exmpl = Linker(model)\n",
    "\n",
    "a = ftm.model.make_entity(\"Person\")\n",
    "a.add(\"name\", \"hans kelsen\")\n",
    "a.add(\"title\", \"Dr\")\n",
    "a.add(\"birthDate\", \"1908-07-06\")\n",
    "a.make_id(\"hans kelsen\")\n",
    "\n",
    "b = ftm.model.make_entity(\"Person\")\n",
    "b.add(\"name\", \"Prof. Dr. hans kelsen\")\n",
    "b.add(\"birthDate\", \"1908\")\n",
    "b.add(\"title\", \"Prof.\")\n",
    "b.make_id(\"Prof. Dr. Hans Kelsen\")\n",
    "\n",
    "match = enricher.make_match(a, b)\n",
    "match.decision = match.SAME\n",
    "linker_exmpl.add(match)\n",
    "\n",
    "merged_ent  = a.merge(b)\n",
    "{\n",
    "    \"a\": a.id,\n",
    "    \"b\": b.id,\n",
    "    \"merged\": merged_ent.id,\n",
    "    \"result\": merged_ent.to_dict()[\"properties\"]}"
   ]
  },
  {
   "source": [
    "## On data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic adapted form https://github.com/alephdata/followthemoney/blob/6cb55e319f69443dff17bf1ee5dd1a37a31b5c4a/followthemoney/cli/dedupe.py\n",
    "\n",
    "linker = Linker(model)\n",
    "for match in matches: \n",
    "    linker.add(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeEntities(inpath, outfile, linker):\n",
    "    infile = open(inpath)\n",
    "\n",
    "    with infile as f:\n",
    "        for line in f:\n",
    "            entity = model.get_proxy(json.loads(line))\n",
    "            applied = linker.apply(entity)\n",
    "            \n",
    "\n",
    "            json_ent = json.dumps(applied.to_dict(), sort_keys=True)\n",
    "            outfile.write(json_ent + \"\\n\")\n",
    "\n",
    "merged_path  = path + \"/nb-merge-output/merged.json\"\n",
    "outfile = open(merged_path,  \"w\")\n",
    "mergeEntities(ep_path , outfile, linker)\n",
    "mergeEntities(ma_path, outfile, linker)\n",
    "merged_aggr_path = path + \"/nb-merge-output/merged_aggr.json\""
   ]
  },
  {
   "source": [
    "### Aggregate CLI command"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$merged_path\" \"$merged_aggr_path\"\n",
    "cat $1 | ftm aggregate -o $2\n"
   ]
  },
  {
   "source": [
    "# CLI\n",
    "I also implemented the merge-file-generator as a command-line tool in a more performant way. The `wd_merge.py` script generates a matching file that links FtM IDs that have a common Wikidata ID and calls `ftm link`, which applies the merges and `ftm aggregate` to actually merge them (see [fragmentation](https://followthemoney.readthedocs.io/en/latest/fragments.html))\n",
    "\n",
    "```\n",
    "python wd_merger.py match-wd -f <infile2> -s <infile1> -o <outfile>\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Due to duplicates within everypolitician regarding Wikidata IDs, we have more matches (685)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 685 matches\n"
     ]
    }
   ],
   "source": [
    "%run -i wd_merger.py match-wd -f data/output/everypolitician.json -s data/output/meineabgeordneten_wikidata.json -o data/output/cli-merge-output/merged.json"
   ]
  },
  {
   "source": [
    "This generates a `matches.json`, which can be used to stream to an Aleph instance or ingest into a Neo4j db.\n",
    "\n",
    "Aleph:\n",
    "```\n",
    "cat matches.json | alephclient --host https://aleph.occrp.org --api-key <api-key> write-entities -f <collection_id> \n",
    "```\n",
    "\n",
    " Neo4j:\n",
    " ```\n",
    " cat matches.json | ftm export-cypher | cypher-shell -u user -p password\n",
    " ```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}